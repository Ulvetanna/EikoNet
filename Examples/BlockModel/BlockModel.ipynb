{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pylab as plt\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import random\n",
    "from glob import glob\n",
    "from radam import RAdam\n",
    "\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "from torch import Tensor\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import SGD, Adam, RMSprop\n",
    "from torch.autograd import Variable, grad\n",
    "from torch.utils.data.sampler import SubsetRandomSampler,WeightedRandomSampler\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import apex.amp as amp\n",
    "import sys\n",
    "import skfmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting random source reciever locations from across the volume. $Xp$ represents a vector of source and reciever locations in form $[Xs,Ys,Zs,Xr,Yr,Zr]$. $Yp$ represents the velocity at the reciever locations. This model represents a central 3D cude with velocity $0.7km/s$ and a background velocity of $0.5km/s$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randPoints(numsamples=10000,dist=None):\n",
    "    # Increasing the number of points to allow for possible mismatc h\n",
    "    X  = np.random.rand(numsamples,6)*2\n",
    "    return X\n",
    "\n",
    "\n",
    "def CreateDataset(PATH,create=False,Numsamples=5000,PointDistance=None):\n",
    "    if create == True:\n",
    "        Xp = randPoints(numsamples=Numsamples,dist=PointDistance)\n",
    "\n",
    "        Yp  = np.ones(Xp.shape[0])*0.5\n",
    "        indR = (Xp[:,3] <= 1.4) & (Xp[:,3] >= 0.6) & (Xp[:,4] <= 1.4) & (Xp[:,4] >= 0.6) & (Xp[:,5] <= 1.4) & (Xp[:,5] >= 0.6)\n",
    "        Yp[indR] = 0.7\n",
    "\n",
    "        # Saving the training dataset\n",
    "        np.save('{}/Xp'.format(PATH),Xp)\n",
    "        np.save('{}/Yp'.format(PATH),Yp)\n",
    "    else:\n",
    "        try:\n",
    "            Xp = np.load('{}/Xp.npy'.format(PATH))\n",
    "            Yp = np.load('{}/Yp.npy'.format(PATH))\n",
    "        except ValueError:\n",
    "            print('Please specify a correct source path, or create a dataset')\n",
    "\n",
    "    return Xp,Yp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Achietecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NN, self).__init__()\n",
    "        # Defining the neural network\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        \n",
    "        # Layers \n",
    "        self.fc0  = Linear(2*3,32)\n",
    "        self.fc1  = Linear(32,512)\n",
    "\n",
    "        # resnet - block 1\n",
    "        self.rn1_fc1  = Linear(512,512)\n",
    "        self.rn1_fc2  = Linear(512,512)\n",
    "        self.rn1_fc3  = Linear(512,512)\n",
    "\n",
    "        # resnet - block 2 \n",
    "        self.rn2_fc1  = Linear(512,512)\n",
    "        self.rn2_fc2  = Linear(512,512)\n",
    "        self.rn2_fc3  = Linear(512,512)\n",
    "\n",
    "        # resnet - block 2 \n",
    "        self.rn3_fc1  = Linear(512,512)\n",
    "        self.rn3_fc2  = Linear(512,512)\n",
    "        self.rn3_fc3  = Linear(512,512)\n",
    "\n",
    "        # resnet - block 2 \n",
    "        self.rn4_fc1  = Linear(512,512)\n",
    "        self.rn4_fc2  = Linear(512,512)\n",
    "        self.rn4_fc3  = Linear(512,512)\n",
    "\n",
    "        # resnet - block 2 \n",
    "        self.rn5_fc1  = Linear(512,512)\n",
    "        self.rn5_fc2  = Linear(512,512)\n",
    "        self.rn5_fc3  = Linear(512,512)\n",
    "\n",
    "        # resnet - block 2 \n",
    "        self.rn6_fc1  = Linear(512,512)\n",
    "        self.rn6_fc2  = Linear(512,512)\n",
    "        self.rn6_fc3  = Linear(512,512)\n",
    "\n",
    "        # resnet - block 2 \n",
    "        self.rn7_fc1  = Linear(512,512)\n",
    "        self.rn7_fc2  = Linear(512,512)\n",
    "        self.rn7_fc3  = Linear(512,512)\n",
    "\n",
    "        # resnet - block 2 \n",
    "        self.rn8_fc1  = Linear(512,512)\n",
    "        self.rn8_fc2  = Linear(512,512)\n",
    "        self.rn8_fc3  = Linear(512,512)\n",
    "\n",
    "                # resnet - block 2 \n",
    "        self.rn9_fc1  = Linear(512,512)\n",
    "        self.rn9_fc2  = Linear(512,512)\n",
    "        self.rn9_fc3  = Linear(512,512)\n",
    "\n",
    "        # resnet - block 2 \n",
    "        self.rn10_fc1  = Linear(512,512)\n",
    "        self.rn10_fc2  = Linear(512,512)\n",
    "        self.rn10_fc3  = Linear(512,512)\n",
    "\n",
    "\n",
    "        # Output structure\n",
    "        self.fc8  = Linear(512,32)\n",
    "        self.fc9  = Linear(32,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "         ============= Neural Network Structure ============\n",
    "            INPUTS:  [Xs,Ys,Zs,Xr,Yr,Zr] - Shape (1x6)\n",
    "                   Xs,Ys,Zs  -- the X,Y,Z location \n",
    "                   of the first point defined the source\n",
    "                   Xr,Yr,Zr  -- the X,Y,Z location \n",
    "                   of the second point defined the reciever\n",
    "                   \n",
    "                   \n",
    "            OUTPUTS: [Tsr,Trs,Tss,Trr] - Shape (1x4)\n",
    "                     Tsr - Travel-time to reciever from source\n",
    "                    \n",
    "                     dT - [dT_sr_s,dT_sr_r] - Shape (1x2)\n",
    "                     dT_sr_s - gradient of Tsr with respect to source   ([Xs,Ys,Zs]) - shape (1x3)\n",
    "                     dT_sr_r - gradient of Tsr with respect to reciever ([Xr,Yr,Zr]) - shape (1x3)\n",
    "        '''\n",
    "        \n",
    "        xor = x\n",
    "        x   = self.relu(self.fc0(x))\n",
    "        x   = self.relu(self.fc1(x))\n",
    "\n",
    "        # Resnet - Block 1\n",
    "        x0  = x\n",
    "        x   = self.relu(self.rn1_fc1(x))\n",
    "        x   = self.relu(self.rn1_fc3(x) + self.rn1_fc2(x0))\n",
    "\n",
    "\n",
    "        # Resnet - Block 2\n",
    "        x0  = x\n",
    "        x   = self.relu(self.rn2_fc1(x))\n",
    "        x   = self.relu(self.rn2_fc3(x)+self.rn2_fc2(x0))\n",
    "\n",
    "        # Resnet - Block 3\n",
    "        x0  = x\n",
    "        x   = self.relu(self.rn3_fc1(x))\n",
    "        x   = self.relu(self.rn3_fc3(x)+self.rn3_fc2(x0))\n",
    "\n",
    "        # Resnet - Block 4\n",
    "        x0  = x\n",
    "        x   = self.relu(self.rn4_fc1(x))\n",
    "        x   = self.relu(self.rn4_fc3(x)+self.rn4_fc2(x0))\n",
    "\n",
    "        # Resnet - Block 5\n",
    "        x0  = x\n",
    "        x   = self.relu(self.rn5_fc1(x))\n",
    "        x   = self.relu(self.rn5_fc3(x)+self.rn5_fc2(x0))\n",
    "\n",
    "        # Resnet - Block 6\n",
    "        x0  = x\n",
    "        x   = self.relu(self.rn6_fc1(x))\n",
    "        x   = self.relu(self.rn6_fc3(x)+self.rn6_fc2(x0))\n",
    "\n",
    "        # Resnet - Block 5\n",
    "        x0  = x\n",
    "        x   = self.relu(self.rn7_fc1(x))\n",
    "        x   = self.relu(self.rn7_fc3(x)+self.rn7_fc2(x0))\n",
    "\n",
    "        # Resnet - Block 5\n",
    "        x0  = x\n",
    "        x   = self.relu(self.rn8_fc1(x))\n",
    "        x   = self.relu(self.rn8_fc3(x)+self.rn8_fc2(x0))\n",
    "\n",
    "        # Resnet - Block 5\n",
    "        x0  = x\n",
    "        x   = self.relu(self.rn9_fc1(x))\n",
    "        x   = self.relu(self.rn9_fc3(x)+self.rn9_fc2(x0))\n",
    "\n",
    "        # Resnet - Block 5\n",
    "        x0  = x\n",
    "        x   = self.relu(self.rn10_fc1(x))\n",
    "        x   = self.relu(self.rn10_fc3(x)+self.rn10_fc2(x0))\n",
    "\n",
    "\n",
    "        # Joining two blocks\n",
    "        x     = self.relu(self.fc8(x))\n",
    "        tau   = abs(self.fc9(x))\n",
    "        dtau, = torch.autograd.grad(outputs=tau, inputs=xor, grad_outputs=torch.ones(tau.size()).to(device), \n",
    "                                  only_inputs=True,create_graph=True,retain_graph=True)\n",
    "\n",
    "\n",
    "        T0 = torch.sqrt(((xor[:,3]-xor[:,0])**2 + (xor[:,4]-xor[:,1])**2 + (xor[:,5]-xor[:,2])**2))\n",
    "        T1 = (T0**2)*(dtau[:,3]**2 + dtau[:,4]**2 + dtau[:,5]**2)\n",
    "        T2 = 2*tau[:,0]*(dtau[:,3]*(xor[:,3]-xor[:,0]) + dtau[:,4]*(xor[:,4]-xor[:,1]) + dtau[:,5]*(xor[:,5]-xor[:,2]))\n",
    "        T3 = tau[:,0]**2\n",
    "        S2 = (T1+T2+T3)\n",
    "        V2 = torch.sqrt(1/S2)\n",
    "        T  = T0*tau[:,0]\n",
    "\n",
    "        return T, V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, data, target, transform=None):\n",
    "        # Creating identical pairs\n",
    "        self.data    = Variable(Tensor(data))\n",
    "        self.target  = Variable(Tensor(target))\n",
    "\n",
    "    def send_device(self,device):\n",
    "        self.data    = self.data.to(device)\n",
    "        self.target  = self.target.to(device)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.target[index]\n",
    "        return x, y, index\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == torch.nn.Linear:\n",
    "        stdv = (1. / math.sqrt(m.weight.size(1))/1.)*2\n",
    "        m.weight.data.uniform_(-stdv,stdv)\n",
    "        m.bias.data.uniform_(-stdv,stdv)\n",
    "\n",
    "def EikonalLoss(Yobs,Ypred):\n",
    "        diff  = abs(Yobs-Ypred)\n",
    "        loss  = torch.mean((Yobs-Ypred)**2)\n",
    "        return loss, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, network, optimizer,scheduler, model_path, model_name):\n",
    "        self.network    = network\n",
    "        self.optimizer  = optimizer\n",
    "        self.model_path = model_path\n",
    "        self.model_name = model_name\n",
    "        self.scheduler  = scheduler\n",
    "\n",
    "        self.total_train_loss = []\n",
    "        self.total_val_loss   = []\n",
    "        \n",
    "    def train(self, dataset,n_epochs,ResampleBounds):\n",
    "        from torch.autograd import Variable\n",
    "        import time\n",
    "\n",
    "        len_dataset         = len(dataset)\n",
    "        n_batches           = int(len(dataset)/btsize + 1)\n",
    "        training_start_time = time.time()\n",
    "\n",
    "        # Sending the data to GPU\n",
    "        dataset.send_device(device) \n",
    "\n",
    "        # --------- Splitting the dataset into training and validation -------\n",
    "        indices            = list(range(int(len_dataset)))\n",
    "        validation_idx     = np.random.choice(indices, size=int(len_dataset*(validation_per/100)), replace=False)\n",
    "        train_idx          = list(set(indices) - set(validation_idx))\n",
    "        validation_sampler = SubsetRandomSampler(validation_idx)\n",
    "        train_sampler      = SubsetRandomSampler(train_idx)\n",
    "\n",
    "        train_loader       = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=btsize,\n",
    "            sampler=train_sampler,\n",
    "            )    \n",
    "        validation_loader  = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=btsize,\n",
    "            sampler=validation_sampler,\n",
    "        )    \n",
    "\n",
    "\n",
    "\n",
    "        # Defining the initial weights to sample by\n",
    "        weights = Tensor(torch.ones(len(dataset))).to(device)\n",
    "        weights[validation_idx] = 0.0\n",
    "\n",
    "        print(\"=======================================================================================\")\n",
    "        print(\"=======================================================================================\")\n",
    "        print(\"========================== Eikonal Solver - Training ==================================\")\n",
    "        print(\"=======================================================================================\")\n",
    "        print(\"=======================================================================================\")\n",
    "        print(\"        Num Epochs          = {}\".format(n_epochs))\n",
    "        print(\"        Num Batches         = {}\".format(n_batches))\n",
    "        print(\"=======================================================================================\")\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            print_every           = 1\n",
    "            start_time            = time.time()\n",
    "            running_sample_count  = 0\n",
    "\n",
    "            total_train_loss      = 0\n",
    "            total_val_loss        = 0\n",
    "\n",
    "\n",
    "\n",
    "            # --- Defining the weighting of the samples\n",
    "\n",
    "\n",
    "            weights                 = torch.clamp(weights/weights.max(),ResampleBounds[0],ResampleBounds[1])\n",
    "            weights[validation_idx] = 0.0\n",
    "            train_sampler_wei       = WeightedRandomSampler(weights, len(weights), replacement=True)\n",
    "            train_loader_wei        = torch.utils.data.DataLoader(\n",
    "                                        dataset,\n",
    "                                        batch_size=btsize,\n",
    "                                        sampler=train_sampler_wei,\n",
    "                                      )\n",
    "\n",
    "            weights                 = Tensor(torch.zeros(len(dataset))).to(device)\n",
    "            \n",
    "            for i, data in enumerate(train_loader_wei, 0):\n",
    "                #print('----------------- Epoch {} - Batch {} --------------------'.format(epoch,i))\n",
    "                \n",
    "                # Get inputs/outputs and wrap in variable object\n",
    "                inputs, labels, indexbatch = data\n",
    "\n",
    "                # Making sure input and labels are floats\n",
    "                inputs.float()\n",
    "                labels.float()\n",
    "                \n",
    "                # Setting so the inputs require a gradient\n",
    "                inputs.requires_grad_()\n",
    "\n",
    "                # Forward pass\n",
    "                outputs,pred_labels = self.network(inputs)\n",
    "                \n",
    "                # Determining the loss from the system \n",
    "                loss_, wv  = EikonalLoss(labels,pred_labels)\n",
    "\n",
    "                with amp.scale_loss(loss_, optimizer) as loss_value:\n",
    "                  loss_value.backward()\n",
    "\n",
    "                # Update parameters\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # Updating the weights\n",
    "                weights[indexbatch] = wv\n",
    "\n",
    "                del inputs, labels, indexbatch, outputs, pred_labels, loss_, wv\n",
    "\n",
    "\n",
    "\n",
    "            # ----- Determining the Validation Loss -----\n",
    "            for i, data_train in enumerate(train_loader, 0):\n",
    "                inputs_train, labels_train, indexbatch_train = data_train\n",
    "                inputs_train.requires_grad_()\n",
    "                outputs_train,pred_labels_train = self.network(inputs_train)\n",
    "                train_loss,wv               = EikonalLoss(labels_train,pred_labels_train)\n",
    "                total_train_loss           += train_loss.item()\n",
    "                del inputs_train, labels_train, indexbatch_train, outputs_train, pred_labels_train, train_loss, wv\n",
    "\n",
    "\n",
    "            # ----- Determining the Training Loss -----\n",
    "            for i, data_val in enumerate(validation_loader, 0):\n",
    "                inputs_val, labels_val, indexbatch_val = data_val\n",
    "                inputs_val.requires_grad_()\n",
    "                outputs_val,pred_labels_val = self.network(inputs_val)\n",
    "                val_loss,wv                 = EikonalLoss(labels_val,pred_labels_val)\n",
    "                total_val_loss             += val_loss.item()\n",
    "                del inputs_val, labels_val, indexbatch_val, outputs_val, pred_labels_val, val_loss, wv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # Creating a running loss for both training and validation data\n",
    "            total_val_loss   /= len(validation_loader)\n",
    "            total_train_loss /= len(train_loader)\n",
    "            self.total_train_loss.append(total_train_loss)\n",
    "            self.total_val_loss.append(total_val_loss)\n",
    "            self.scheduler.step(total_val_loss)\n",
    "\n",
    "            del train_loader_wei,train_sampler_wei\n",
    "\n",
    "            if (epoch+1) % 10 == 1:\n",
    "                with torch.no_grad():\n",
    "                    print(\"Epoch = {} -- Training loss = {:.4e} -- Validation loss = {:.4e}\".format(epoch+1,total_train_loss,total_val_loss))\n",
    "\n",
    "            if (epoch+1) % 200 == 1:\n",
    "                with torch.no_grad():\n",
    "                    torch.save({\n",
    "                        'epoch':epoch,\n",
    "                        'model_state_dict': self.network.state_dict(),\n",
    "                        'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                        'train_loss': self.total_train_loss,\n",
    "                        'val_loss': self.total_val_loss,\n",
    "                        }, '{}/{}_{}_{}.pt'.format(self.model_path, self.model_name,str(epoch).zfill(5),total_val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs         = 200\n",
    "datatset_size      = 1e6\n",
    "validation_per     = 10\n",
    "btsize             = 752\n",
    "GPUid              = 7\n",
    "ResampleBounds    = [0.1,0.9]\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "PATH               = './OutputModels'\n",
    "Xp,Yp              = CreateDataset(PATH,Numsamples=int(datatset_size),create=True)\n",
    "dataset            = NumpyDataset(Xp,Yp)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "device             = torch.device(\"cuda:{}\".format(GPUid))\n",
    "\n",
    "# --------- Defining the neural network -------\n",
    "torch.cuda.set_device(device)\n",
    "net = NN()\n",
    "net.apply(init_weights)\n",
    "net.float()\n",
    "net.to(device)\n",
    "\n",
    "# --------- Defining optimizer -------\n",
    "optimizer  = torch.optim.Adam(net.parameters(),lr=5e-5)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "net, optimizer = amp.initialize(net, optimizer, opt_level='O0')\n",
    "\n",
    "# --------- Training Model -------\n",
    "model = Model(net, optimizer,scheduler,model_path=PATH,model_name='Homogenous_Model')\n",
    "model.train(dataset,num_epochs,ResampleBounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting\n",
    "\n",
    "A series of script to load up the saved EikoNet model and compute the travel-time to series of reciver locations in the X-Z domain at $Y=1$, at $0.01km$ spacing, from some user defined source location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(filepath,device):\n",
    "    '''\n",
    "        Loading an instance of EikoNet from a saved .pt\n",
    "    \n",
    "    \n",
    "        INPUTS:\n",
    "            filepath - path to .pt file\n",
    "            \n",
    "        OUTPUTS:\n",
    "            model - Eikonet Model\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    checkpoint = torch.load(filepath)\n",
    "    # Defining Neural Network\n",
    "    net = NN().to(device)\n",
    "    net.apply(init_weights)\n",
    "    net.float()\n",
    "    net.to(device)\n",
    "    optimizer  = torch.optim.Adam(net.parameters())\n",
    "    scheduler = []\n",
    "    model = Model(net, optimizer,scheduler, model_path=filepath,model_name='')\n",
    "    model.total_train_loss = checkpoint['train_loss']\n",
    "    model.total_val_loss   = checkpoint['val_loss']\n",
    "\n",
    "    model.network.load_state_dict(checkpoint['model_state_dict'])\n",
    "    for parameter in model.network.parameters():\n",
    "        parameter.requires_grad = False\n",
    "\n",
    "    model.network.eval()\n",
    "    return model\n",
    "\n",
    "def Plotting(model,Xsrc):\n",
    "    # ================= Computing the expected travel-time and grads ===================\n",
    "    spc      = 0.005\n",
    "    linsp    = (np.arange(0,1,spc) + spc/2)*2\n",
    "    X,Y      = np.meshgrid(linsp,linsp)\n",
    "\n",
    "    XP       = np.ones((len(X.flatten()),6))\n",
    "    XP[:,:3] = Xsrc\n",
    "    XP[:,3]  = X.flatten()\n",
    "    XP[:,4]  = 1.\n",
    "    XP[:,5]  = Y.flatten()\n",
    "\n",
    "    # Determining the travel-time for the points\n",
    "    XP = Variable(Tensor(XP)).to(device)\n",
    "    XP.requires_grad_()\n",
    "    tt,vv  = model.network(XP)\n",
    "\n",
    "\n",
    "    # Determining the travel-time for the points\n",
    "    TT = tt.to('cpu').data.numpy().reshape(X.shape)\n",
    "    V  = vv.to('cpu').data.numpy().reshape(X.shape)\n",
    "\n",
    "    Vobs = np.ones(X.shape)*0.5\n",
    "    indmin=np.argmin(abs(linsp-0.6))\n",
    "    indmax=np.argmin(abs(linsp-1.4))\n",
    "    Vobs[indmin:indmax,indmin:indmax] = 0.7\n",
    "\n",
    "    # Create prediceted travel-time for the 1D model\n",
    "    phi    = np.ones(X.shape)\n",
    "    phi[np.argmin(abs(linsp-Xsrc[0])),np.argmin(abs(linsp-Xsrc[2]))] = -1\n",
    "    TTobs = np.transpose(skfmm.travel_time(phi,Vobs,dx=spc*2))\n",
    "\n",
    "    # --- Plotting the expected travel-time\n",
    "    plt.clf();plt.close('all')\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    ax = fig.add_subplot(331)  \n",
    "    quad1 = ax.pcolormesh(X,Y,Vobs,vmin=0.1,vmax=0.9)\n",
    "    ax.contour(X,Y,TT,25,colors='w')\n",
    "    plt.colorbar(quad1,ax=ax, orientation=\"horizontal\", pad=0.1, label='Travel-Time')\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title('1D Travel=-time')\n",
    "    ax.set_ylabel('Y location')\n",
    "    ax.set_xlabel('X location')\n",
    "    ax.set_xlim([0.,2.])\n",
    "    ax.set_ylim([0.,2.])\n",
    "\n",
    "    ax = fig.add_subplot(332)  \n",
    "    quad1 = ax.pcolormesh(X,Y,V,vmin=0.1,vmax=0.9)\n",
    "    ax.contour(X,Y,TT,np.arange(0,6,0.3),colors='w')\n",
    "    plt.colorbar(quad1,ax=ax, orientation=\"horizontal\", pad=0.1, label='Obs Velocity')\n",
    "    ax.set_title('Recovered Velocity')\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_ylabel('Y location')\n",
    "    ax.set_xlabel('X location')\n",
    "    ax.set_xlim([0.,2.])\n",
    "    ax.set_ylim([0.,2.])\n",
    "\n",
    "    ax = fig.add_subplot(333)  \n",
    "    quad1 = ax.pcolormesh(X,Y,((V-Vobs)/Vobs)*100,cmap='bwr',vmin=-20.,vmax=20)\n",
    "    plt.colorbar(quad1,ax=ax, orientation=\"horizontal\", pad=0.1, label='Pred Velocity')\n",
    "    ax.set_title('Percentage Difference')\n",
    "    ax.contour(X,Y,Vobs,colors='k',linewidths=0.1)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_ylabel('Y location')\n",
    "    ax.set_xlabel('X location')\n",
    "    ax.set_xlim([0.,2.])\n",
    "    ax.set_ylim([0.,2.])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ================= Computing the expected travel-time and grads ===================\n",
    "    spc      = 0.005\n",
    "    linsp    = (np.arange(0,1,spc) + spc/2)*2\n",
    "    X,Y      = np.meshgrid(linsp,linsp)\n",
    "\n",
    "    XP       = np.ones((len(X.flatten()),6))\n",
    "    XP[:,:3] = Xsrc\n",
    "    XP[:,3]  = X.flatten()\n",
    "    XP[:,4]  = Y.flatten()\n",
    "    XP[:,5]  = 1.\n",
    "\n",
    "\n",
    "    # Determining the travel-time for the points\n",
    "    XP = Variable(Tensor(XP)).to(device)\n",
    "    XP.requires_grad_()\n",
    "    tt,vv  = model.network(XP)\n",
    "\n",
    "\n",
    "    # Determining the travel-time for the points\n",
    "    TT = tt.to('cpu').data.numpy().reshape(X.shape)\n",
    "    V  = vv.to('cpu').data.numpy().reshape(X.shape)\n",
    "    Vobs = np.ones(X.shape)*0.5\n",
    "    indmin=np.argmin(abs(linsp-0.6))\n",
    "    indmax=np.argmin(abs(linsp-1.4))\n",
    "    Vobs[indmin:indmax,indmin:indmax] = 0.7\n",
    "\n",
    "\n",
    "    Xm,Ym,Zm  = np.meshgrid(linsp,linsp,linsp)\n",
    "    phi       = np.ones(Xm.shape)\n",
    "    Vobs_3d   = np.ones(Xm.shape)*0.5\n",
    "    indmin=np.argmin(abs(linsp-0.6))\n",
    "    indmax=np.argmin(abs(linsp-1.4))\n",
    "    Vobs_3d[indmin:indmax,indmin:indmax,indmin:indmax] = 0.7\n",
    "    phi[np.argmin(abs(linsp-Xsrc[0])),np.argmin(abs(linsp-Xsrc[1])),np.argmin(abs(linsp-Xsrc[2]))] = -1\n",
    "    TTobs = skfmm.travel_time(phi,Vobs_3d,dx=spc*2)\n",
    "    TTobs = TTobs[:,:,np.argmin(abs(linsp-1.))]\n",
    "    #print(TTobs)\n",
    "    del Vobs_3d,Xm,Ym,Zm,phi\n",
    "\n",
    "\n",
    "    ax = fig.add_subplot(334)  \n",
    "    quad1 = ax.pcolormesh(X,Y,Vobs,vmin=0.1,vmax=0.9)\n",
    "    cbar = ax.contour(X,Y,TTobs,np.arange(0,6,0.3),colors='w')\n",
    "    plt.colorbar(quad1,ax=ax, orientation=\"horizontal\", pad=0.1, label='Travel-Time')\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title('1D Travel=-time')\n",
    "    ax.set_ylabel('Y location')\n",
    "    ax.set_xlabel('X location')\n",
    "    ax.set_xlim([0.,2.])\n",
    "    ax.set_ylim([0.,2.])\n",
    "    ax.clabel(cbar)\n",
    "    #ax.grid()\n",
    "\n",
    "\n",
    "    ax = fig.add_subplot(335)  \n",
    "    quad1 = ax.pcolormesh(X,Y,V,vmin=0.1,vmax=0.9)\n",
    "    ax.contour(X,Y,TT,np.arange(0,6,0.3),colors='w')\n",
    "    plt.colorbar(quad1,ax=ax, orientation=\"horizontal\", pad=0.1, label='Obs Velocity')\n",
    "    ax.set_title('Recoverd Velocity')\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_ylabel('Y location')\n",
    "    ax.set_xlabel('X location')\n",
    "    ax.set_xlim([0.,2.])\n",
    "    ax.set_ylim([0.,2.])\n",
    "\n",
    "    ax = fig.add_subplot(336)  \n",
    "    quad1 = ax.pcolormesh(X,Y,((V-Vobs)/Vobs)*100,cmap='bwr',vmin=-20.,vmax=20.)\n",
    "    plt.colorbar(quad1,ax=ax, orientation=\"horizontal\", pad=0.1, label='Pred Velocity')\n",
    "    ax.set_title('Percentage Difference')\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_ylabel('Y location')\n",
    "    ax.set_xlabel('X location')\n",
    "    ax.set_xlim([0.,2.])\n",
    "    ax.set_ylim([0.,2.])\n",
    "\n",
    "\n",
    "    # -- Plotting the training loss \n",
    "    ax  = fig.add_subplot(325)\n",
    "    ax.set_title('Loss Terms')\n",
    "\n",
    "    ax.plot(np.arange(len(model.total_train_loss))+1,model.total_train_loss,'r',label='Training-Loss')\n",
    "    ax.set_xlim([1,len(np.arange(len(model.total_train_loss))+1)])\n",
    "    ax.set_ylabel('Training Loss')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_yscale('log')\n",
    "    ax.plot(np.arange(len(model.total_train_loss))+1,model.total_val_loss,'k',label='Validation-Loss')\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_ylim([1e-4,1e-2])\n",
    "    ax.legend(loc=0)\n",
    "    ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.suptitle('Training-Loss ={}, Validation-Loss={}\\n'.format(model.total_train_loss[-1],model.total_val_loss[-1]))\n",
    "\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = './OutputModels/Block_Model_00195_0.0004759901660561029.pt'\n",
    "md    = load_checkpoint(fname,device)\n",
    "fig   = Plotting(md,[1.0,1.,0.1])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
